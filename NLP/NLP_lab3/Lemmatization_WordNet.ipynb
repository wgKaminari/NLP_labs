{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c7bf663",
   "metadata": {},
   "source": [
    "# Lemmatization/WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d407420",
   "metadata": {},
   "source": [
    "# Task 0. \n",
    "Execute the notebook and complete listed exercises (between CODE_START and CODE_END blocks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0d8ba",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320c7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22319f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b844db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spiritual Ritual Festival (Népal)\\nBeginning of Line-up :)\\nIt is left for the line-up (y)\\nSee more at:... http://t.co/QMNz62OEuc'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "positive_tweets[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ac77f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spiritual', 'Ritual', 'Festival', '(', 'Népal', ')', 'Beginning', 'of', 'Line-up', ':)', 'It', 'is', 'left', 'for', 'the', 'line-up', '(', 'y', ')', 'See', 'more', 'at', ':', '...', 'http://t.co/QMNz62OEuc']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bff5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4f767dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spiritual', 'JJ'),\n",
       " ('Ritual', 'NNP'),\n",
       " ('Festival', 'NNP'),\n",
       " ('(', '('),\n",
       " ('Népal', 'NNP'),\n",
       " (')', ')'),\n",
       " ('Beginning', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Line-up', 'NNP'),\n",
       " (':)', 'NNP'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('left', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('line-up', 'NN'),\n",
       " ('(', '('),\n",
       " ('y', 'NN'),\n",
       " (')', ')'),\n",
       " ('See', 'VB'),\n",
       " ('more', 'JJR'),\n",
       " ('at', 'IN'),\n",
       " (':', ':'),\n",
       " ('...', ':'),\n",
       " ('http://t.co/QMNz62OEuc', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[27])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67021a",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03de2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4556edaf",
   "metadata": {},
   "source": [
    "## Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7abd728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets:, [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')]------------[Synset('borsch.n.01')]\n",
      "lemma names:, ['car', 'auto', 'automobile', 'machine', 'motorcar']------------['borsch', 'borsh', 'borscht', 'borsht', 'borshch', 'bortsch']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word_synset = wn.synsets(\"car\")\n",
    "word_synset_b = wn.synsets(\"borsch\")\n",
    "print(f\"synsets:, {word_synset}------------{word_synset_b}\")\n",
    "print(f\"lemma names:, {word_synset[0].lemma_names()}------------{word_synset_b[0].lemma_names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f6a2aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a motor vehicle with four wheels; usually propelled by an internal combustion engine',\n",
       " 'a Russian or Polish soup usually containing beet juice as a foundation')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].definition(), word_synset_b[0].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219f949",
   "metadata": {},
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6053729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['he needs a car to get to work'], [])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].examples(), word_synset_b[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f7203e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wheeled vehicle adapted to the rails of railroad'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e514107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['three cars had jumped the rails']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[1].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321d5bb8",
   "metadata": {},
   "source": [
    "![Alt text](hypernyms-hyponyms-explained-image-a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3313744e",
   "metadata": {},
   "source": [
    "## Hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36858da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beach_wagon.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('used-car.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('ambulance.n.01'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be069d25",
   "metadata": {},
   "source": [
    "## Hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7969aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_synset[0].hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e97d7c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entity.n.01', 'physical_entity.n.01', 'object.n.01', 'whole.n.02', 'living_thing.n.01', 'organism.n.01', 'plant.n.02', 'vascular_plant.n.01', 'woody_plant.n.01', 'tree.n.01']\n"
     ]
    }
   ],
   "source": [
    "tree = wn.synsets(\"tree\")[0]\n",
    "paths = tree.hypernym_paths()\n",
    "for p in paths:\n",
    "  print([synset.name() for synset in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40739f6a",
   "metadata": {},
   "source": [
    "## Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aca48a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('trunk.n.01'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('crown.n.07')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ad37c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.substance_meronyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d534c",
   "metadata": {},
   "source": [
    "## Holonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f91808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.member_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70178e",
   "metadata": {},
   "source": [
    "![Alt text](2.avif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d34e5c",
   "metadata": {},
   "source": [
    "## Lemmatization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74bfa481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e36c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4363e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e0eeb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'child', 'were', 'driving', 'car']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_sentence(tokens):\n",
    "  lemmatized_sentence = []\n",
    "\n",
    "  # CODE_START\n",
    "  for token in tokens:\n",
    "      lemma = lemmatizer.lemmatize(token)\n",
    "      lemmatized_sentence.append(lemma) \n",
    "  # CODE_END\n",
    "\n",
    "  return lemmatized_sentence\n",
    "tokens = [\"The\", \"children\", \"were\", \"driving\", \"cars\"]\n",
    "lemmatize_sentence(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bfb9d",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1226725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3e8abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "a\n",
      "about\n",
      "above\n",
      "after\n",
      "again\n",
      "against\n",
      "ain\n",
      "all\n",
      "am\n",
      "an\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99e10171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('NN'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('VB'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('JJ'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('RB'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token_lower = token.lower()\n",
    "        # Delete URLs and mentions\n",
    "        if re.match(r'^https?://', token_lower) or token_lower.startswith('@'):\n",
    "            continue\n",
    "        # Delete stop words and punctuation\n",
    "        if token_lower in stop_words or token_lower in string.punctuation:\n",
    "            continue\n",
    "        # Lemmatize the token\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token_lower, pos)\n",
    "        cleaned_tokens.append(lemmatized_token)\n",
    "\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bcefb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d918fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE_START\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n",
    "\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fb7c342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f866c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "  # CODE_START\n",
    "  for tokens in cleaned_tokens_list:\n",
    "    for token in tokens:\n",
    "      yield token\n",
    "  # CODE_END\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc13f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 383), ('follow', 362), ('u', 360), ('love', 336), ('...', 290), ('get', 269), ('good', 261)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "# CODE_START\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "# Виведемо 10 найпоширеніших слів\n",
    "print(freq_dist_pos.most_common(10))\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea0db1",
   "metadata": {},
   "source": [
    "# Task 1. \n",
    "Change the code so it removes hashtags during pre-processing. (E.g. #Ukraine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1c4217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Tokens No Tags: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "def process_tokens_no_tags(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('NN'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('VB'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('JJ'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('RB'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token_lower = token.lower()\n",
    "        # Delete URLs and mentions\n",
    "        if re.match(r'^https?://', token_lower) or token_lower.startswith('@') or token_lower.startswith('#'): # Added condition to remove hashtags\n",
    "            continue\n",
    "        # Delete stop words and punctuation\n",
    "        if token_lower in stop_words or token_lower in string.punctuation:\n",
    "            continue\n",
    "        # Lemmatize the token\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemmatized_token = lemmatizer.lemmatize(token_lower, pos)\n",
    "        cleaned_tokens.append(lemmatized_token)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\"Process Tokens No Tags:\", process_tokens_no_tags(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32388b17",
   "metadata": {},
   "source": [
    "# Task 2. \n",
    "Modify process_tokens() so that instead of using lemmatizer.lemmatize(), it will use WordNet synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c70ce2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Tokens Wordnet: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "def process_tokens_WordNet(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('NN'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('VB'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('JJ'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('RB'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN  # Default to noun\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token_lower = token.lower()\n",
    "\n",
    "        # Remove URLs, mentions, hashtags\n",
    "        if re.match(r'^https?://', token_lower) or token_lower.startswith('@') or token_lower.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Remove stopwords and punctuation\n",
    "        if token_lower in stop_words or token_lower in string.punctuation:\n",
    "            continue\n",
    "\n",
    "        # Get POS for WordNet\n",
    "        pos = get_wordnet_pos(tag)\n",
    "\n",
    "        # Try getting the base lemma from WordNet synsets\n",
    "        synsets = wordnet.synsets(token_lower, pos=pos)\n",
    "        if synsets:\n",
    "            # Get the first lemma name of the first synset\n",
    "            base_form = synsets[0].lemmas()[0].name()\n",
    "        else:\n",
    "            base_form = token_lower  # fallback if no synset found\n",
    "\n",
    "        cleaned_tokens.append(base_form)\n",
    "\n",
    "    return cleaned_tokens\n",
    "\n",
    "print(\"Process Tokens Wordnet:\", process_tokens_WordNet(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1d99c",
   "metadata": {},
   "source": [
    "# Task 3. \n",
    "Let’s suppose that semantic distance between words is the distance to the common semantic parent (hypernym). Write a function that will compute this distance between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1b54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Distance: 17\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def semantic_distance(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "\n",
    "    if not synsets1 or not synsets2:\n",
    "        return float('no match')  # If word not found in WordNet\n",
    "\n",
    "    syn1 = synsets1[0]\n",
    "    syn2 = synsets2[0]\n",
    "\n",
    "    # Find the lowest common hypernym\n",
    "    common_hypernyms = syn1.lowest_common_hypernyms(syn2)\n",
    "\n",
    "    if not common_hypernyms:\n",
    "        return float('no common hypernyms')  # If no common hypernym is found\n",
    "\n",
    "    common_hyper = common_hypernyms[0]\n",
    "\n",
    "    # Calculate the distance from each synset to the common hypernym\n",
    "    dist1 = syn1.shortest_path_distance(common_hyper)\n",
    "    dist2 = syn2.shortest_path_distance(common_hyper)\n",
    "\n",
    "    if dist1 is None or dist2 is None:\n",
    "        return float('inf')\n",
    "\n",
    "    return dist1 + dist2\n",
    "print(f\"Semantic Distance: {semantic_distance('cat','car')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
